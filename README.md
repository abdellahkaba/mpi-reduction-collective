# MPI Collective Reduction - Calcul Parall√®le avec Docker

![MPI](https://img.shields.io/badge/MPI-Parallel%20Computing-blue)
![Docker](https://img.shields.io/badge/Docker-Containerized-green)
![Performance](https://img.shields.io/badge/Speedup-3.62x-brightgreen)

## üìå Pr√©sentation

Ce projet d√©montre une impl√©mentation de **r√©duction collective MPI** pour effectuer des calculs intensifs sur un cluster de machines. Il compare les performances entre :

- Une version **monoprocesseur** (s√©quentielle)
- Une version **parall√®le** utilisant 4 n≈ìuds MPI

L'approche utilise :
- `MPI_Scatterv` pour une distribution √©quilibr√©e des donn√©es
- `MPI_Reduce` pour la r√©duction finale
- Un cluster Docker pour l'ex√©cution distribu√©e

## üß† Explication Technique

### Algorithme de R√©duction
```python
Donn√©es initiales (N √©l√©ments)
       ‚Üì
MPI_Scatterv ‚Üí Distribution vers P processus
       ‚Üì
Calcul intensif en parall√®le (10k op√©rations/√©l√©ment)
       ‚Üì
MPI_Reduce ‚Üí Somme globale
Un exemple de r√©duction collective MPI avec distribution intelligente des donn√©es et calcul intensif, ex√©cut√© dans un cluster Docker.

```
### üõ† Installation & Ex√©cution
```python
    1. Lancer le cluster MPI:
        docker compose up --build -d
        
    2. Se connecter au n≈ìud ma√Ætre: 
        docker exec -it --user vagrant mpi-node1 bash
        
    3. Compiler le programme:
        mpicc collective_reduction.c -o collective_reduction -lm
        
    4. Ex√©cuter en parall√®le (4 processus):
        mpirun -np 4 -host mpi-node1,mpi-node2,mpi-node3,mpi-node4 ./collective_reduction

    5. Comparer avec la version s√©quentielle:
        mpirun -np 1 ./collective_reduction
```  
### üìä R√©sultats de Performance

```python
## Comparaison Monoprocesseur vs Parall√®le (4 n≈ìuds)

| M√©trique               | Parall√®le (4x) | S√©quentiel | Acc√©l√©ration |
|------------------------|---------------|------------|--------------|
| Temps total            | 0.201s        | 0.730s     | 3.62x        |
| Temps calcul moyen     | 0.191s        | 0.730s     | -            |
| Temps distribution     | 0.002s        | N/A        | -            |
| Somme finale           | 818.36        | 818.36     | -            |

**Efficacit√© parall√®le**: 90.5% (3.62x/4.00x th√©orique)

```
### Sortie Type

```plaintext
=== EX√âCUTION PARALL√àLE ===
[COLLECTIVE] Distribution des donn√©es...
[PROCESSUS 0] D√©but du calcul sur 250 √©l√©ments...
[PROCESSUS 0] Calcul termin√© en 0.194s, somme locale: 204.59
[COLLECTIVE] Somme globale finale: 818.36
Temps total: 0.201s
